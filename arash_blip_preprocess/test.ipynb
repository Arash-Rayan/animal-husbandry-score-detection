{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4acd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9e71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_float_data(data_path:str): \n",
    "    clean_score =[]\n",
    "    videos = os.listdir(data_path)\n",
    "    for vid in videos : \n",
    "        try: \n",
    "            score = vid.split('-')[-1].replace('.mp4', '')\n",
    "            clean_score.append((vid , float(score)))\n",
    "        except: \n",
    "            os.remove(os.path.join(data_path, vid))\n",
    "            continue\n",
    "    return clean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211e5dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2-1-2.25-3.mp4', 3.0),\n",
       " ('2-10-3-3.mp4', 3.0),\n",
       " ('2-11-3.5-2.7.mp4', 2.7),\n",
       " ('2-12-3.8-2.75.mp4', 2.75),\n",
       " ('2-13-3.5-2.75.mp4', 2.75),\n",
       " ('2-14-3-3.mp4', 3.0),\n",
       " ('2-15-3.8-3.mp4', 3.0),\n",
       " ('2-16-3-3.mp4', 3.0),\n",
       " ('2-17-3-3.mp4', 3.0),\n",
       " ('2-18-3.8-3.mp4', 3.0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = clean_and_float_data('data')\n",
    "clean_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3510f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "def capture_and_save_frames(data_path:tuple , save_dir:str) :  \n",
    " vids_name = os.listdir(data_path)\n",
    " vids_path = [os.path.join(data_path, name) for name in vids_name]\n",
    "\n",
    " name_counter = 0\n",
    " for name ,vid  in zip(vids_name, vids_path): \n",
    "    # i used encoded video to capture all frames of the video at once in a sequnce \n",
    "    video = EncodedVideo.from_path(vid)\n",
    "    clip = video.get_clip(0 , 1)['video']\n",
    "    frames_path = os.path.join(save_dir, name)\n",
    "    os.makedirs(frames_path, exist_ok=True)\n",
    "    frames = [clip[:,i,:,:] for i in range((clip.shape)[1])] # i seperetaed each frame since its sequence [3, 25 ,...] to 25 one frames \n",
    "    \n",
    "    to_pil = ToPILImage() # to pil image only understand normalized video for saving my frames thats where /255 came from\n",
    "    for frame in frames : \n",
    "      to_pil(frame/255).save(os.path.join(frames_path , f'{name_counter}.jpg'))\n",
    "      name_counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22900d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture_and_save_frames('data', save_dir='frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms \n",
    "\n",
    "class MyDataSet(Dataset): \n",
    "    def __init__(self , root_dir:str):\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = self.clean_and_float_data()\n",
    "\n",
    "\n",
    "    def clean_and_float_data(self): \n",
    "            clean_score =[]\n",
    "            videos = os.listdir(self.root_dir)\n",
    "            for vid in videos : \n",
    "                try: \n",
    "                    score = vid.split('-')[-1].replace('.mp4', '')\n",
    "                    clean_score.append((vid , float(score)))\n",
    "                except: \n",
    "                    os.remove(os.path.join(self.root_dir, vid))\n",
    "                    continue\n",
    "            return clean_score\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "         img , score = self.samples[index]\n",
    "         images_dir = os.path.join(self.root_dir , img)\n",
    "         img_names = os.listdir(images_dir)\n",
    "         img_paths = [os.path.join(images_dir , fname) for fname in img_names]\n",
    "         frames = [Image.open(p).convert(\"RGB\") for p in img_paths]\n",
    "         return frames , score, img  # from what i got , blip2 understand raw image no frames no tensor no array\n",
    "\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd33714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Image.open('frames/2-1-2.25-3.mp4/0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73a397f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataSet('frames') \n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c8e79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_size  = int(.80 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_set , test_set = random_split(dataset , [train_size , test_size])\n",
    "len(train_set) , len(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c192e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 2.75\n"
     ]
    }
   ],
   "source": [
    "for images, score in test_set: \n",
    "    print(len(images), score)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fe79340",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ScoringMLP(nn.Module):\n",
    "    def __init__(self , in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_channel, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_channel)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.model import load_model_and_preprocess\n",
    "\n",
    "model, vis_processors, _ = load_model_and_preprocess(\n",
    "    name=\"blip2_feature_extractor\", model_type=\"pretrain\", is_eval=True, device=device\n",
    ")\n",
    "\n",
    "for images, score , folder_name in dataset:  # 25 frame on each ne \n",
    "    counter = 0\n",
    "    frame_features = []\n",
    "    for frame in images: \n",
    "        image_tensor = vis_processors[\"eval\"](frame).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            features = model.visual_encoder(image_tensor) #[1, 257, 1408] [batch, patches , vectors]\n",
    "            cls_token = features[:, 0, :]            # [1, 1408]\n",
    "            frame_features.append(cls_token.squeeze(0))  # [1408]\n",
    "            score_tensor = torch.tensor([score], dtype=torch.float32)\n",
    "\n",
    "    video_tensor = torch.stack(frame_features)     # [25, 1408]\n",
    "    counter+=1\n",
    "\n",
    "    save_path = os.path.join('frames', folder_name , 'feature.pt')\n",
    "    torch.save(video_tensor, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
